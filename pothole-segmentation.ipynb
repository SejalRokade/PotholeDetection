{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7309233,"sourceType":"datasetVersion","datasetId":3887050},{"sourceId":8064046,"sourceType":"datasetVersion","datasetId":4757275},{"sourceId":11215094,"sourceType":"datasetVersion","datasetId":7003273},{"sourceId":11226465,"sourceType":"datasetVersion","datasetId":7011820},{"sourceId":318292,"sourceType":"modelInstanceVersion","modelInstanceId":268589,"modelId":289608}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/pothole-image-segmentation-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:02:00.424127Z","iopub.execute_input":"2025-04-02T15:02:00.424426Z","iopub.status.idle":"2025-04-02T15:02:03.874736Z","shell.execute_reply.started":"2025-04-02T15:02:00.4244Z","shell.execute_reply":"2025-04-02T15:02:03.868211Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1.setup ","metadata":{}},{"cell_type":"code","source":"# Install Ultralytics library\n!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:25:55.574682Z","iopub.execute_input":"2025-04-02T17:25:55.575043Z","iopub.status.idle":"2025-04-02T17:26:00.7298Z","shell.execute_reply.started":"2025-04-02T17:25:55.575013Z","shell.execute_reply":"2025-04-02T17:26:00.72875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Disable warnings in the notebook to maintain clean output cells\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import necessary libraries\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport cv2\nimport yaml\nfrom PIL import Image\nfrom collections import deque\nfrom ultralytics import YOLO\nfrom IPython.display import Video","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:02.79968Z","iopub.execute_input":"2025-04-02T17:26:02.800035Z","iopub.status.idle":"2025-04-02T17:26:05.819534Z","shell.execute_reply.started":"2025-04-02T17:26:02.800007Z","shell.execute_reply":"2025-04-02T17:26:05.818862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure the visual appearance of Seaborn plots\nsns.set(rc={'axes.facecolor': '#ffe4de'}, style='darkgrid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:08.150044Z","iopub.execute_input":"2025-04-02T17:26:08.150572Z","iopub.status.idle":"2025-04-02T17:26:08.155669Z","shell.execute_reply.started":"2025-04-02T17:26:08.150528Z","shell.execute_reply":"2025-04-02T17:26:08.154687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2.Load YOLOv8-seg Model**","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained YOLOv8 nano segmentation model\nmodel = YOLO('yolov8n-seg.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:10.915097Z","iopub.execute_input":"2025-04-02T17:26:10.915417Z","iopub.status.idle":"2025-04-02T17:26:12.609604Z","shell.execute_reply.started":"2025-04-02T17:26:10.915393Z","shell.execute_reply":"2025-04-02T17:26:12.608498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.Dataset Exploration**","metadata":{}},{"cell_type":"code","source":"# Define the dataset_path\ndataset_path = '/kaggle/input/pothole-image-segmentation-dataset'\n\n# Set the path to the YAML file\nyaml_file_path = os.path.join(dataset_path, '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/data.yaml')\n\n# Load and print the contents of the YAML file\nwith open(yaml_file_path, 'r') as file:\n    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n    print(yaml.dump(yaml_content, default_flow_style=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:15.960631Z","iopub.execute_input":"2025-04-02T17:26:15.960968Z","iopub.status.idle":"2025-04-02T17:26:15.977938Z","shell.execute_reply.started":"2025-04-02T17:26:15.960944Z","shell.execute_reply":"2025-04-02T17:26:15.977013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set paths for training and validation image sets\ntrain_images_path = os.path.join(dataset_path, '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/train', 'images')\nvalid_images_path = os.path.join(dataset_path, '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/valid', 'images')\n\n# Initialize counters for the number of images\nnum_train_images = 0\nnum_valid_images = 0\n\n# Initialize sets to hold the unique sizes of images\ntrain_image_sizes = set()\nvalid_image_sizes = set()\n\n# Check train images sizes and count\nfor filename in os.listdir(train_images_path):\n    if filename.endswith('.jpg'):  \n        num_train_images += 1\n        image_path = os.path.join(train_images_path, filename)\n        with Image.open(image_path) as img:\n            train_image_sizes.add(img.size)\n\n# Check validation images sizes and count\nfor filename in os.listdir(valid_images_path):\n    if filename.endswith('.jpg'): \n        num_valid_images += 1\n        image_path = os.path.join(valid_images_path, filename)\n        with Image.open(image_path) as img:\n            valid_image_sizes.add(img.size)\n\n# Print the results\nprint(f\"Number of training images: {num_train_images}\")\nprint(f\"Number of validation images: {num_valid_images}\")\n\n# Check if all images in training set have the same size\nif len(train_image_sizes) == 1:\n    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\nelse:\n    print(\"Training images have varying sizes.\")\n\n# Check if all images in validation set have the same size\nif len(valid_image_sizes) == 1:\n    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\nelse:\n    print(\"Validation images have varying sizes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:19.63434Z","iopub.execute_input":"2025-04-02T17:26:19.634746Z","iopub.status.idle":"2025-04-02T17:26:24.102795Z","shell.execute_reply.started":"2025-04-02T17:26:19.634711Z","shell.execute_reply":"2025-04-02T17:26:24.101944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the seed for the random number generator\nrandom.seed(0)\n\n# Create a list of image files\nimage_files = [f for f in os.listdir(train_images_path) if f.endswith('.jpg')]\n\n# Randomly select 15 images\nrandom_images = random.sample(image_files, 15)\n\n# Create a new figure\nplt.figure(figsize=(19, 12))\n\n# Loop through each image and display it in a 3x5 grid\nfor i, image_file in enumerate(random_images):\n    image_path = os.path.join(train_images_path, image_file)\n    image = Image.open(image_path)\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(image)\n    plt.axis('off')\n# Add a suptitle\nplt.suptitle('Random Selection of Dataset Images', fontsize=24)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n# Deleting unnecessary variable to free up memory\ndel image_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:30.865586Z","iopub.execute_input":"2025-04-02T17:26:30.865952Z","iopub.status.idle":"2025-04-02T17:26:34.240039Z","shell.execute_reply.started":"2025-04-02T17:26:30.865925Z","shell.execute_reply":"2025-04-02T17:26:34.238723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4.Fine-tuning yolov8n**","metadata":{}},{"cell_type":"code","source":"import torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Device Count:\", torch.cuda.device_count())\nprint(\"CUDA Version:\", torch.version.cuda)\nprint(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:45.938793Z","iopub.execute_input":"2025-04-02T17:26:45.939116Z","iopub.status.idle":"2025-04-02T17:26:45.974203Z","shell.execute_reply.started":"2025-04-02T17:26:45.939094Z","shell.execute_reply":"2025-04-02T17:26:45.973427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = 0 if torch.cuda.is_available() else \"cpu\"  # Auto-select GPU if available\n\nresults = model.train(\n    data=yaml_file_path,\n    epochs=15,\n    imgsz=640,\n    patience=15,\n    batch=16,\n    optimizer='auto',\n    lr0=0.0001,\n    lrf=0.01,\n    dropout=0.25,\n    device=device,  # Auto GPU/CPU selection\n    seed=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:26:49.189859Z","iopub.execute_input":"2025-04-02T17:26:49.190185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5.Model Performance Evaluation**","metadata":{}},{"cell_type":"code","source":"# Define the path to the directory\npost_training_files_path = '/kaggle/working/runs/segment/train'\n\n# List the files in the directory\n!ls {post_training_files_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:10:01.121123Z","iopub.execute_input":"2025-04-02T15:10:01.121487Z","iopub.status.idle":"2025-04-02T15:10:01.302664Z","shell.execute_reply.started":"2025-04-02T15:10:01.121454Z","shell.execute_reply":"2025-04-02T15:10:01.301745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Validation performance metric evaluation","metadata":{}},{"cell_type":"code","source":"# Construct the path to the best model weights file using os.path.join\nbest_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n\n# Load the best model weights into the YOLO model\nbest_model = YOLO(best_model_path)\n\n# Validate the best model using the validation set with default parameters\nmetrics = best_model.val(split='val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:10:39.612803Z","iopub.execute_input":"2025-04-02T15:10:39.613196Z","iopub.status.idle":"2025-04-02T15:10:47.796202Z","shell.execute_reply.started":"2025-04-02T15:10:39.613169Z","shell.execute_reply":"2025-04-02T15:10:47.795199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the dictionary to a pandas DataFrame and use the keys as the index\nmetrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n\n# Display the DataFrame\nmetrics_df.round(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:10:54.169567Z","iopub.execute_input":"2025-04-02T15:10:54.169919Z","iopub.status.idle":"2025-04-02T15:10:54.192298Z","shell.execute_reply.started":"2025-04-02T15:10:54.169871Z","shell.execute_reply":"2025-04-02T15:10:54.191579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model testing**","metadata":{}},{"cell_type":"markdown","source":"****6.1 Test on validation image**###### **","metadata":{},"attachments":{}},{"cell_type":"code","source":"# Define the path to the validation images\nvalid_images_path = os.path.join(dataset_path, '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/valid', 'images')\n\n# List all jpg images in the directory\nimage_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n\n# Select 9 images at equal intervals\nnum_images = len(image_files)\nselected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n\n# Initialize the subplot\nfig, axes = plt.subplots(3, 3, figsize=(20, 21))\nfig.suptitle('Validation Set Inferences', fontsize=24)\n\n# Perform inference on each selected image and display it\nfor i, ax in enumerate(axes.flatten()):\n    image_path = os.path.join(valid_images_path, selected_images[i])\n    results = best_model.predict(source=image_path, imgsz=640)\n    annotated_image = results[0].plot()\n    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n    ax.imshow(annotated_image_rgb)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:11:16.913502Z","iopub.execute_input":"2025-04-02T15:11:16.913834Z","iopub.status.idle":"2025-04-02T15:11:20.664356Z","shell.execute_reply.started":"2025-04-02T15:11:16.91381Z","shell.execute_reply":"2025-04-02T15:11:20.662785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the path to the sample video in the dataset\ndataset_video_path = '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/sample_video.mp4'\n\n# Define the destination path in the working directory\nvideo_path = '/kaggle/working/sample_video.mp4'\n\n# Copy the video file from its original location in the dataset to the current working directory in Kaggle\nshutil.copyfile(dataset_video_path, video_path)\n\n# Initiate vehicle detection on the sample video using the best performing model and save the output\nbest_model.predict(source=video_path, save=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:12:47.43795Z","iopub.execute_input":"2025-04-02T15:12:47.438291Z","iopub.status.idle":"2025-04-02T15:12:59.861228Z","shell.execute_reply.started":"2025-04-02T15:12:47.438268Z","shell.execute_reply":"2025-04-02T15:12:59.860242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n!ffmpeg -y -loglevel panic -i /kaggle/working/runs/segment/predict/sample_video.avi processed_sample_video.mp4\n\n# Embed and display the processed sample video within the notebook\nVideo(\"processed_sample_video.mp4\", embed=True, width=960)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:13:11.880631Z","iopub.execute_input":"2025-04-02T15:13:11.88103Z","iopub.status.idle":"2025-04-02T15:13:26.05716Z","shell.execute_reply.started":"2025-04-02T15:13:11.880995Z","shell.execute_reply":"2025-04-02T15:13:26.055244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Save the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export the model\nbest_model.export(format='onnx')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:15:34.766779Z","iopub.execute_input":"2025-04-02T15:15:34.767286Z","iopub.status.idle":"2025-04-02T15:15:34.823441Z","shell.execute_reply.started":"2025-04-02T15:15:34.767242Z","shell.execute_reply":"2025-04-02T15:15:34.822181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Realtime Road damage assessment**","metadata":{}},{"cell_type":"code","source":"# Define the path to the validation images\nvalid_images_path = os.path.join(dataset_path, '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/valid', 'images')\n\n# List all jpg images in the directory\nimage_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n\n# Select a sample image\nselected_image = image_files[30]\n\n# Perform inference on the selected image\nimage_path = os.path.join(valid_images_path, selected_image)\nresults = best_model.predict(source=image_path, imgsz=640, conf=0.5)\nannotated_image = results[0].plot()\nannotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n\n# Determine the number of subplots needed (1 original + number of masks)\nnum_subplots = 1 + (len(results[0].masks.data) if results[0].masks is not None else 0)\n\n# Initialize the subplot with 1 row and n columns\nfig, axes = plt.subplots(1, num_subplots, figsize=(15, 5))\n\n# Display the original annotated image\naxes[0].imshow(annotated_image_rgb)\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\n# If multiple masks, iterate and display each mask\nif results[0].masks is not None:\n    masks = results[0].masks.data.cpu().numpy()\n    for i, mask in enumerate(masks):\n        # Threshold the mask to make sure it's binary\n        # Any value greater than 0 is set to 255, else it remains 0\n        binary_mask = (mask > 0).astype(np.uint8) * 255\n        axes[i+1].imshow(binary_mask, cmap='gray')\n        axes[i+1].set_title(f'Segmented Mask {i+1}')\n        axes[i+1].axis('off')\n\n# Adjust layout and display the subplot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:18:02.840674Z","iopub.execute_input":"2025-04-02T15:18:02.841055Z","iopub.status.idle":"2025-04-02T15:18:03.848387Z","shell.execute_reply.started":"2025-04-02T15:18:02.841025Z","shell.execute_reply":"2025-04-02T15:18:03.847478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Initialize variables to hold total area and individual areas\ntotal_area = 0\narea_list = []\n\n# Perform operations if masks are available\nif results[0].masks is not None:\n    masks = results[0].masks.data.cpu().numpy()   # Retrieve masks as numpy arrays\n    image_area = masks.shape[1] * masks.shape[2]  # Calculate total number of pixels in the image\n    \n    # Set up the subplot dynamically based on the number of masks\n    fig, axes = plt.subplots(1, len(masks), figsize=(12, 8))\n    \n    # Ensure `axes` is iterable even if there is only one mask\n    if len(masks) == 1:\n        axes = [axes]\n\n    for i, mask in enumerate(masks):\n        binary_mask = (mask > 0).astype(np.uint8) * 255  # Convert mask to binary\n        color_mask = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)  # Convert binary mask to color\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        \n        if contours:  # Ensure that at least one contour is found\n            contour = contours[0]  # Retrieve the first contour\n            area = cv2.contourArea(contour)  # Calculate the area of the pothole\n            area_list.append(area)  # Append area to the list\n            cv2.drawContours(color_mask, [contour], -1, (0, 255, 0), 3)  # Draw the contour on the mask\n\n            # Display the mask with the green contour\n            axes[i].imshow(color_mask)\n            axes[i].set_title(f'Pothole {i+1}')\n            axes[i].axis('off')\n\n    # Display all masks\n    plt.tight_layout()\n    plt.show()\n\n    # Calculate and print areas after displaying the images\n    for i, area in enumerate(area_list):\n        print(f\"Area of Pothole {i+1}: {area} pixels\")  \n        total_area += area  # Sum the areas for total\n\n    # Calculate and print the total damaged area and percentage of road damaged by potholes\n    print(\"-\"*50)\n    print(f\"Total Damaged Area by Potholes: {total_area} pixels\")\n    print(f\"Total Pixels in Image: {image_area} pixels\")\n    print(f\"Percentage of Road Damaged: {(total_area / image_area) * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:18:10.157131Z","iopub.execute_input":"2025-04-02T15:18:10.157614Z","iopub.status.idle":"2025-04-02T15:18:10.959288Z","shell.execute_reply.started":"2025-04-02T15:18:10.157568Z","shell.execute_reply":"2025-04-02T15:18:10.958326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the video path\nvideo_path = '/kaggle/working/sample_video.mp4'\n\n# Define font, scale, colors, and position for the annotation\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfont_scale = 1\ntext_position = (40, 80)\nfont_color = (255, 255, 255)    # White color for text\nbackground_color = (0, 0, 255)  # Red background for text\n\n# Initialize a deque with fixed length for averaging the last 10 percentage damages\ndamage_deque = deque(maxlen=10)\n\n# Open the video\ncap = cv2.VideoCapture(video_path)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('road_damage_assessment.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n# Read until video is completed\nwhile cap.isOpened():\n     # Capture frame-by-frame\n    ret, frame = cap.read()\n    if ret:\n        # Perform inference on the frame\n        results = best_model.predict(source=frame, imgsz=640, conf=0.25)\n        processed_frame = results[0].plot(boxes=False)\n        \n        # Initializes percentage_damage to 0\n        percentage_damage = 0 \n\n         # If masks are available, calculate total damage area and percentage\n        if results[0].masks is not None:\n            total_area = 0\n            masks = results[0].masks.data.cpu().numpy()\n            image_area = frame.shape[0] * frame.shape[1]  # total number of pixels in the image\n            for mask in masks:\n                binary_mask = (mask > 0).astype(np.uint8) * 255\n                contour, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n                total_area += cv2.contourArea(contour[0])\n            \n            percentage_damage = (total_area / image_area) * 100\n\n        # Calculate and update the percentage damage\n        damage_deque.append(percentage_damage)\n        smoothed_percentage_damage = sum(damage_deque) / len(damage_deque)\n            \n        # Draw a thick line for text background\n        cv2.line(processed_frame, (text_position[0], text_position[1] - 10),\n                 (text_position[0] + 350, text_position[1] - 10), background_color, 40)\n        # Annotate the frame with the percentage of damage\n        cv2.putText(processed_frame, f'Road Damage: {smoothed_percentage_damage:.2f}%', text_position, font, font_scale, font_color, 2, cv2.LINE_AA)         \n    \n        # Write the processed frame to the output video\n        out.write(processed_frame)\n        \n        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n        # cv2.imshow('Road Damage Assessment', processed_frame) # Display the processed frame\n        # if cv2.waitKey(1) & 0xFF == ord('q'): # Press Q on keyboard to exit the loop\n        #     break \n    else:\n        break\n\n# Release the video capture and video write objects\ncap.release()\nout.release()\n\n# Close all the frames\n# cv2.destroyAllWindows()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:19:07.69725Z","iopub.execute_input":"2025-04-02T15:19:07.697637Z","iopub.status.idle":"2025-04-02T15:19:19.905959Z","shell.execute_reply.started":"2025-04-02T15:19:07.69761Z","shell.execute_reply":"2025-04-02T15:19:19.905071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n!ffmpeg -y -loglevel panic -i /kaggle/working/road_damage_assessment.avi road_damage_assessment.mp4\n\n# Embed and display the processed sample video within the notebook\nVideo(\"road_damage_assessment.mp4\", embed=True, width=960)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:20:39.427657Z","iopub.execute_input":"2025-04-02T15:20:39.428045Z","iopub.status.idle":"2025-04-02T15:20:52.916379Z","shell.execute_reply.started":"2025-04-02T15:20:39.428017Z","shell.execute_reply":"2025-04-02T15:20:52.914447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q gradio onnxruntime\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:48:16.011449Z","iopub.execute_input":"2025-04-02T16:48:16.011939Z","iopub.status.idle":"2025-04-02T16:48:29.214564Z","shell.execute_reply.started":"2025-04-02T16:48:16.011904Z","shell.execute_reply":"2025-04-02T16:48:29.213755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:49:24.155032Z","iopub.execute_input":"2025-04-02T16:49:24.155332Z","iopub.status.idle":"2025-04-02T16:49:24.160172Z","shell.execute_reply.started":"2025-04-02T16:49:24.155309Z","shell.execute_reply":"2025-04-02T16:49:24.159281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport torch\nimport gradio as gr\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom collections import deque\n\n# âœ… Load YOLOv8 model\nmodel_path = \"/kaggle/working/runs/segment/train/weights/best.pt\"\nmodel = YOLO(model_path)\n\n# âœ… Function to process images and analyze damage\ndef analyze_damage(image):\n    orig_image = image.copy()\n    h, w, _ = orig_image.shape\n\n    # Perform inference\n    results = model(image)\n\n    # Draw detections\n    for result in results:\n        for box in result.boxes:\n            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n            score = box.conf[0].item()\n            if score > 0.5:\n                cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(orig_image, f\"Pothole {score:.2f}\", (x1, y1 - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    # âœ… Calculate total damage area\n    total_area = 0\n    image_area = h * w  # Total image pixels\n\n    if hasattr(results[0], 'masks') and results[0].masks is not None:\n        masks = results[0].masks.data.cpu().numpy()\n        for mask in masks:\n            binary_mask = (mask > 0).astype(np.uint8) * 255\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if contours:\n                total_area += cv2.contourArea(contours[0])\n\n    damage_percentage = (total_area / image_area) * 100 if image_area > 0 else 0\n\n    return orig_image, f\"ğŸš§ Road Damage: {damage_percentage:.2f}% ğŸš§\"\n\n# âœ… Function to process videos\ndef process_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter('/kaggle/working/road_damage_output.avi', fourcc, 20.0,\n                          (int(cap.get(3)), int(cap.get(4))))\n    \n    damage_deque = deque(maxlen=10)\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Run inference\n        results = model(frame)\n        processed_frame = results[0].plot(boxes=False)\n\n        # Calculate road damage\n        total_area = 0\n        image_area = frame.shape[0] * frame.shape[1]\n\n        if results[0].masks is not None:\n            masks = results[0].masks.data.cpu().numpy()\n            for mask in masks:\n                binary_mask = (mask > 0).astype(np.uint8) * 255\n                contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n                if contours:\n                    total_area += cv2.contourArea(contours[0])\n\n        damage_percentage = (total_area / image_area) * 100 if image_area > 0 else 0\n        damage_deque.append(damage_percentage)\n        smoothed_damage = sum(damage_deque) / len(damage_deque)\n\n        # Overlay damage percentage\n        cv2.putText(processed_frame, f'Road Damage: {smoothed_damage:.2f}%', (40, 80),\n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n        \n        out.write(processed_frame)\n\n    cap.release()\n    out.release()\n    return \"/kaggle/working/road_damage_output.avi\"\n\n# ğŸ¨ **Enhanced Gradio UI**\nwith gr.Blocks(title=\"Road Damage Assessment\") as app:\n    gr.Markdown(\"<h1 style='text-align: center; color: #4CAF50;'>ğŸš§ Road Damage Detection & Assessment ğŸš§</h1>\")\n    gr.Markdown(\"### Upload an image or video to analyze road damage and pothole severity.\")\n\n    with gr.Tab(\"ğŸ“¸ Image Analysis\"):\n        with gr.Row():\n            input_image = gr.Image(type=\"numpy\", label=\"Upload Road Image\")\n            analyze_button = gr.Button(\"ğŸ” Analyze Damage\")\n        with gr.Row():\n            output_image = gr.Image(type=\"numpy\", label=\"Damage Analysis Output\")\n            damage_text = gr.Textbox(label=\"Damage Percentage\", interactive=False)\n        analyze_button.click(analyze_damage, inputs=input_image, outputs=[output_image, damage_text])\n\n    with gr.Tab(\"ğŸ¥ Video Analysis\"):\n        with gr.Row():\n            input_video = gr.Video(label=\"Upload Road Video\")\n            analyze_video_button = gr.Button(\"ğŸ¬ Process Video\")\n        with gr.Row():\n            output_video = gr.Video(label=\"Processed Video Output\")\n        analyze_video_button.click(process_video, inputs=input_video, outputs=output_video)\n\n# ğŸš€ Launch UI\napp.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:57:29.119694Z","iopub.execute_input":"2025-04-02T17:57:29.120031Z","iopub.status.idle":"2025-04-02T17:57:32.365597Z","shell.execute_reply.started":"2025-04-02T17:57:29.120007Z","shell.execute_reply":"2025-04-02T17:57:32.364719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport torch\nimport gradio as gr\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom collections import deque\n\n# âœ… Load YOLOv8 model\nmodel_path = \"/kaggle/working/runs/segment/train/weights/best.pt\"\nmodel = YOLO(model_path)\n\n# âœ… Function to process images and analyze damage\ndef analyze_damage(image):\n    orig_image = image.copy()\n    h, w, _ = orig_image.shape\n\n    # Perform inference\n    results = model(image)\n\n    # Draw detections\n    for result in results:\n        for box in result.boxes:\n            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n            score = box.conf[0].item()\n            if score > 0.5:\n                cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(orig_image, f\"Pothole {score:.2f}\", (x1, y1 - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    # âœ… Calculate total damage area\n    total_area = 0\n    image_area = h * w  # Total image pixels\n\n    if hasattr(results[0], 'masks') and results[0].masks is not None:\n        masks = results[0].masks.data.cpu().numpy()\n        for mask in masks:\n            binary_mask = (mask > 0).astype(np.uint8) * 255\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if contours:\n                total_area += cv2.contourArea(contours[0])\n\n    damage_percentage = (total_area / image_area) * 100 if image_area > 0 else 0\n\n    return orig_image, f\"ğŸš§ Road Damage: {damage_percentage:.2f}% ğŸš§\"\n\n# âœ… Function to process videos\ndef process_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter('/kaggle/working/road_damage_output.avi', fourcc, 20.0,\n                          (int(cap.get(3)), int(cap.get(4))))\n    \n    damage_deque = deque(maxlen=10)\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Run inference\n        results = model(frame)\n        processed_frame = results[0].plot(boxes=False)\n\n        # Calculate road damage\n        total_area = 0\n        image_area = frame.shape[0] * frame.shape[1]\n\n        if results[0].masks is not None:\n            masks = results[0].masks.data.cpu().numpy()\n            for mask in masks:\n                binary_mask = (mask > 0).astype(np.uint8) * 255\n                contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n                if contours:\n                    total_area += cv2.contourArea(contours[0])\n\n        damage_percentage = (total_area / image_area) * 100 if image_area > 0 else 0\n        damage_deque.append(damage_percentage)\n        smoothed_damage = sum(damage_deque) / len(damage_deque)\n\n        # Overlay damage percentage\n        cv2.putText(processed_frame, f'Road Damage: {smoothed_damage:.2f}%', (40, 80),\n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n        \n        out.write(processed_frame)\n\n    cap.release()\n    out.release()\n    return \"/kaggle/working/road_damage_output.avi\"\n\n# ğŸ¨ **Enhanced Gradio UI**\nwith gr.Blocks(title=\"Road Damage Assessment\") as app:\n    gr.Markdown(\"\"\"\n        # ğŸš§ Road Damage Detection & Assessment ğŸš§\n        Upload an image or video to analyze road damage and pothole severity.\n    \"\"\")\n\n    with gr.Tab(\"ğŸ“¸ Image Analysis\"):\n        gr.Markdown(\"### Upload an image to detect potholes and assess damage severity.\")\n        with gr.Row():\n            input_image = gr.Image(type=\"numpy\", label=\"Upload Road Image\")\n            analyze_button = gr.Button(\"ğŸ” Analyze Damage\", variant=\"primary\")\n        with gr.Row():\n            output_image = gr.Image(type=\"numpy\", label=\"Damage Analysis Output\")\n            damage_text = gr.Textbox(label=\"Damage Percentage\", interactive=False)\n        analyze_button.click(analyze_damage, inputs=input_image, outputs=[output_image, damage_text])\n\n    with gr.Tab(\"ğŸ¥ Video Analysis\"):\n        gr.Markdown(\"### Upload a video for real-time road damage detection.\")\n        with gr.Row():\n            input_video = gr.Video(label=\"Upload Road Video\")\n            analyze_video_button = gr.Button(\"ğŸ¬ Process Video\", variant=\"primary\")\n        with gr.Row():\n            output_video = gr.Video(label=\"Processed Video Output\")\n        analyze_video_button.click(process_video, inputs=input_video, outputs=output_video)\n\n# ğŸš€ Launch UI\napp.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:01:08.324017Z","iopub.execute_input":"2025-04-02T18:01:08.324382Z","iopub.status.idle":"2025-04-02T18:01:11.30949Z","shell.execute_reply.started":"2025-04-02T18:01:08.324347Z","shell.execute_reply":"2025-04-02T18:01:11.308519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}